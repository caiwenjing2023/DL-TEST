import os
import tarfile
import shutil
INDEX_PATH = os.path.join('trec07p', 'delay', 'index')
# INDEX_PATH = os.path.join('trec07p', 'delay', 'index')  # 从性能考虑，先使用较小的数据集进行训练
DATA_PATH = os.path.join('trec07p', 'data')  # 数据文件夹路径
SPAM_PATH = os.path.join('dataset', 'spam')  # 垃圾邮件文件夹路径
HAM_PATH = os.path.join('dataset', 'ham')  # 非垃圾邮件文件夹路径
def create_dataset(index_path, data_path, spam_path, ham_path):
    with open(index_path) as f:
        while True:
            line = f.readline()
            if not line: break
            line = line.split(' ')
            if not os.path.isdir(spam_path):
                os.makedirs(spam_path)
            if not os.path.isdir(ham_path):
                os.makedirs(ham_path)
            label, filename = line[0], line[1].strip('\n').split('/')[-1]
            if label == 'spam':
                shutil.copyfile(os.path.join(data_path, filename), os.path.join(spam_path, filename))
            else:
                shutil.copyfile(os.path.join(data_path, filename), os.path.join(ham_path, filename))

# create_dataset(INDEX_PATH, DATA_PATH, SPAM_PATH, HAM_PATH)
ham_filenames = [name for name in sorted(os.listdir(HAM_PATH))]
spam_filenames = [name for name in sorted(os.listdir(SPAM_PATH))]

# print(len(ham_filenames), len(spam_filenames))

from email import policy, parser

def load_email(filename, file_path):
    with open(os.path.join(file_path, filename), 'rb') as f:
        return parser.BytesParser(policy=policy.default).parse(f)
ham_emails = [load_email(name, HAM_PATH) for name in ham_filenames]
spam_emails = [load_email(name, SPAM_PATH) for name in spam_filenames]
# print(ham_emails[0].get_content().strip())
#
# print(spam_emails[5].get_content().strip())

from collections import Counter

def get_email_structure(email):
    if isinstance(email, str):
        return email
    payload = email.get_payload()
    if isinstance(payload, list):
        return 'multipart({})'.format(', '.join([
            get_email_structure(sub_email)
            for sub_email in payload
        ]))
    else:
        return email.get_content_type()

def structures_counter(emails):
    structures = Counter()
    for email in emails:
        structure = get_email_structure(email)
        structures[structure] += 1
    return structures
# print(structures_counter(ham_emails).most_common())
# print(structures_counter(spam_emails).most_common())



import numpy as np
from sklearn.model_selection import train_test_split

rng = np.random.RandomState(0)

x = np.array(ham_emails + spam_emails, dtype=object)
y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=rng)
# X_train,X_test, y_train, y_test =train_test_split(train_data,train_target,test_size=0.25, random_state=0,stratify=y)
import re
from html import unescape

def html_to_plain_text(html):
    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)
    text = re.sub(r'<[aA]\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)
    text = re.sub(r'<img\s.*?>', ' IMAGE ', text, flags=re.M | re.S | re.I)
    text = re.sub('<.*?>', '', text, flags=re.M | re.S)
    text = re.sub(r'(\s*\n)+', '\n', text, flags=re.M | re.S)
    return unescape(text)


html_spam_emails = [email for email in x_train[y_train==1] if get_email_structure(email) == 'text/html']
sample_html_spam = html_spam_emails[0]
# print(sample_html_spam.get_content().strip()[:1000], '...')
# print(html_to_plain_text(sample_html_spam.get_content())[:1000], '...')

def email_to_text(email):
    html = None
    for part in email.walk():
        ctype = part.get_content_type()
        if ctype not in ('text/plain', 'text/html'):
            continue
        try:
            content = part.get_content()
        except LookupError:
            content = str(part.get_payload())
        if ctype == 'text/plain':
            return content
        else:
            html = content
    if html:
        return html_to_plain_text(html)
# print(email_to_text(sample_html_spam)[:100], '...')

import nltk
import urlextract
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.sparse import csr_matrix


class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, lower_case=True, remove_punctuation=True,
                 replace_urls=True, replace_numbers=True, stemming=True):
        self.lower_case = lower_case
        self.remove_punctuation = remove_punctuation
        self.replace_urls = replace_urls
        self.replace_numbers = replace_numbers
        self.stemming = stemming

    def fit(self, x, y=None):
        return self

    def transform(self, x, y=None):
        x_transformed = []
        stemmer = nltk.PorterStemmer()
        extractor = urlextract.URLExtract()
        for email in x:
            text = email_to_text(email) or ' '
            if self.lower_case:
                text = text.lower()
            if self.remove_punctuation:
                text = re.sub(r'\W+', ' ', text, flags=re.M)
            if self.replace_urls:
                urls = list(set(extractor.find_urls(text)))
                urls.sort(key=lambda item: len(item), reverse=True)
                for url in urls:
                    text = text.replace(url, " URL ")
            if self.replace_numbers:
                text = re.sub(r'\d+(?:\.\d*[eE]\d+)?', 'NUMBER', text)
            word_counts = Counter(text.split())
            if self.stemming:
                stemmed_word_counts = Counter()
                for word, count in word_counts.items():
                    stemmed_word = stemmer.stem(word)
                    stemmed_word_counts[stemmed_word] += count
                word_counts = stemmed_word_counts
            x_transformed.append(word_counts)
        return np.array(x_transformed)


class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, vocabulary_size=1000):
        self.vocabulary_size = vocabulary_size
        self.most_common_ = None
        self.vocabulary_ = None

    def fit(self, x, y=None):
        total_count = Counter()
        for word_count in x:
            for word, count in word_count.items():
                total_count[word] += min(count, 10)
        most_common = total_count.most_common()[:self.vocabulary_size]
        self.most_common_ = most_common
        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}
        return self

    def transform(self, x, y=None):
        rows = []
        cols = []
        data = []
        for row, word_count in enumerate(x):
            for word, count in word_count.items():
                rows.append(row)
                cols.append(self.vocabulary_.get(word, 0))
                data.append(count)
        return csr_matrix((data, (rows, cols)), shape=(len(x), self.vocabulary_size + 1))

x_few = x_train[:3]
x_few_wordcounts = EmailToWordCounterTransformer().fit_transform(x_few)
# print(x_few_wordcounts)

from sklearn.pipeline import Pipeline

preprocess_pipeline = Pipeline([
    ("email_to_wordcount", EmailToWordCounterTransformer()),
    ("wordcount_to_vector", WordCounterToVectorTransformer()),
])

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# 使用数据预处理流水线转换训练数据
x_train_transformed = preprocess_pipeline.fit_transform(x_train)


#
# rfc_clf = RandomForestClassifier(random_state=rng)
# rfc_score = cross_val_score(rfc_clf, x_train_transformed, y_train, cv=5, verbose=3)
# rfc_score.mean()
#
# from sklearn.metrics import precision_score, recall_score
#
# x_test_transformed = preprocess_pipeline.transform(x_test)
#
# rfc_clf = RandomForestClassifier(random_state=rng)
# rfc_clf.fit(x_train_transformed, y_train)
#
# y_pred = rfc_clf.predict(x_test_transformed)

# print("Precision: {:.2f}%".format(100 * precision_score(y_test, y_pred)))
# print("Recall: {:.2f}%".format(100 * recall_score(y_test, y_pred)))





from sklearn import naive_bayes as bayes
train,test,trainlabel,testlabel = train_test_split(x_train_transformed, y_train, test_size=0.2, random_state=rng)

# 贝叶斯
clf = bayes.BernoulliNB(alpha=1,binarize=True)
model = clf.fit(train,trainlabel)
print(model.score(test,testlabel))


# if __name__ == '__main__':
#     create_dataset(INDEX_PATH, DATA_PATH, SPAM_PATH, HAM_PATH)

